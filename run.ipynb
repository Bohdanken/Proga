{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from os import sep, remove\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from datetime import date, datetime, timedelta\n",
    "import utils\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "import unicodedata\n",
    "import string\n",
    "from num2words import num2words\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "import pickle\n",
    "import weather\n",
    "from sklearn import preprocessing\n",
    "import scipy.sparse\n",
    "MODEL_FOLDER=\"model\"\n",
    "\n",
    "def get_date_from_user():\n",
    "    today = datetime.combine(date.today(),datetime.min.time())\n",
    "\n",
    "    query = \"\"\n",
    "    while query == \"\":\n",
    "        try:\n",
    "            query = datetime.strptime(input(\"Enter a date (YYYY-MM-DD): \"), \"%Y-%m-%d\")\n",
    "        except:\n",
    "            print(\"Wrong format! Try again\")\n",
    "\n",
    "    while query < datetime(2023, 1, 20) or query > today:\n",
    "        print(\"Failure!\")\n",
    "        try:\n",
    "            query = datetime.strptime(input(\"Enter a date (YYYY-MM-DD): \"), \"%Y-%m-%d\")\n",
    "        except:\n",
    "            print(\"Wrong format! Try again\")\n",
    "    time_query = \"\"\n",
    "    while time_query == \"\":\n",
    "        try:\n",
    "            time_query = datetime.strptime(input(\"Enter time (HH:MM): \"), \"%H:%M\")\n",
    "        except:\n",
    "            print(\"Wrong format! Try again\")\n",
    "\n",
    "    return datetime.combine(query.date(),time_query.time())\n",
    "\n",
    "def save_page(url, file_name):\n",
    "    page = requests.get(url)\n",
    "    #with open('D:\\Python project\\\\'+file_name +\".html\", \"w\") as f:\n",
    "       # f.write(url+'\\n')\n",
    "    with open(file_name +\".html\", \"wb+\") as f:\n",
    "        f.write(page.content)\n",
    "        f.close()\n",
    "        return file_name +\".html\"\n",
    "\n",
    "def text_processing(file_name):\n",
    "    d={}\n",
    "    with open(file_name, \"r\",encoding='utf-8') as cfile:\n",
    "        #url=cfile.readline()\n",
    "        parsed_html= BeautifulSoup(cfile.read(), features=\"html.parser\")\n",
    "        title = parsed_html.head. find('title').text\n",
    "        textS_title = parsed_html.body.find('h1', attrs={\"id\":'page-title'}).text\n",
    "        text_main = parsed_html.body.find('div', attrs={'class': 'field field-name-body field-type-text-with-summary field-label-hidden'})\n",
    "        d={\"date\":date,\n",
    "           #\"url\":url,\n",
    "           \"title\": title,\n",
    "           \"text_title\":textS_title,\n",
    "           \"text_main\":text_main}\n",
    "    cfile.close()\n",
    "    pd.DataFrame(d,index=[0]).to_csv(\"temp.csv\")\n",
    "    return \"temp.csv\"\n",
    "\n",
    "def remove_names_and_date(page_html_text):\n",
    "    parsed_html = BeautifulSoup(page_html_text, features=\"html.parser\")\n",
    "    p_lines = parsed_html.findAll('p')\n",
    "\n",
    "    min_sentense_word_count = 13\n",
    "    p_index = 0\n",
    "\n",
    "    #find first long sentense\n",
    "    for p_line in p_lines:\n",
    "\n",
    "        strong_lines = p_line.findAll('strong')\n",
    "        if not strong_lines:\n",
    "            continue\n",
    "\n",
    "        for s in strong_lines:\n",
    "            if len(s.text.split(\" \")) >= min_sentense_word_count:\n",
    "                break\n",
    "        else:\n",
    "            p_index += 1\n",
    "            continue\n",
    "        break\n",
    "    for i in range(0, p_index):\n",
    "        page_html_text = page_html_text.replace(str(p_lines[i]), \"\")\n",
    "\n",
    "    return page_html_text\n",
    "\n",
    "def remove_special_characters(data):\n",
    "    result = unicodedata.normalize(\"NFKD\", data)\n",
    "    #CHARS_TO_REMOVE=['\\r','\\n']\n",
    "    return result.replace('\\r', ' ').replace('\\n', ' ')\n",
    "\n",
    "def remove_any_punct(q):\n",
    "    return q.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "\n",
    "def remove_one_letter_word (data):\n",
    "    word_tokens = nltk.word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in word_tokens:\n",
    "        if len(w) == 1 :\n",
    "            continue\n",
    "\n",
    "        new_text= new_text + \" \" + w\n",
    "    return new_text\n",
    "\n",
    "def convert_numbers(data):\n",
    "    tokens = nltk.word_tokenize(str(data))\n",
    "    result=\"\"\n",
    "    for w in tokens :\n",
    "        if w.isdigit():\n",
    "            if(int(w)>10000000000):\n",
    "                continue\n",
    "            w = remove_any_punct(num2words(w))\n",
    "        result = result + ' ' + w\n",
    "    return result\n",
    "\n",
    "def remove_stopwords(data):\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    word_tokens = nltk.word_tokenize(str(data))\n",
    "    stop_stop_words = {\"no\", \"not\"}\n",
    "    stop_words = set(stopwords) - stop_stop_words\n",
    "    result = \"\"\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            result = result + \" \" + w\n",
    "    return result\n",
    "\n",
    "def stemming(data):\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    tokens = nltk.word_tokenize(str(data))\n",
    "    result = \"\"\n",
    "    for w in tokens:\n",
    "        result = result + \" \" + stemmer.stem(w)\n",
    "    return result\n",
    "\n",
    "def lemmatizing(data):\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    tokens = nltk.word_tokenize(str(data))\n",
    "    result = \"\"\n",
    "    for w in tokens:\n",
    "        result = result + \" \" + lemmatizer.lemmatize(w)\n",
    "    return result\n",
    "\n",
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "\n",
    "    for idx, score in sorted_items:\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "    results = {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    return results\n",
    "\n",
    "def conver_doc_to_vector(doc, cv, tfidf):\n",
    "    feature_names = cv.get_feature_names_out()\n",
    "    top_n = 100\n",
    "    tf_idf_vector = tfidf.transform(cv.transform([doc]))\n",
    "\n",
    "    sorted_items = sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "    keywords = extract_topn_from_vector(feature_names, sorted_items, top_n)\n",
    "\n",
    "    return keywords\n",
    "\n",
    "def get_region_from_user():\n",
    "    city = input(\"Enter the name of the region's capital (for example: Poltava or Полтава): \")\n",
    "    while ((city not in df_regions[\"center_city_ua\"].values) and (city not in df_regions[\"center_city_en\"].values)):\n",
    "        print(\"This is not a region's capital in Ukraine. Try again!\")\n",
    "        city = input(\"Enter the name of the region's capital (for example: Poltava or Полтава): \")\n",
    "    index = df_regions.index[df_regions[\"center_city_en\"] == city].tolist()[0] if city in df_regions[\"center_city_en\"].values else df_regions.index[df_regions[\"center_city_ua\"] == city].tolist()[0]\n",
    "    return df_regions.iloc[index] #.at[\"center_city_en\"]\n",
    "\n",
    "def merge_weather_isw_region(df_weather, df_isw, df_region):\n",
    "    fields_to_exlude = [\n",
    "    \"city_resolvedAddress\",\n",
    "    \"day_datetime\",\n",
    "    \"day_datetimeEpoch\",\n",
    "    \"hour_datetime\",\n",
    "    \"hour_datetimeEpoch\",\n",
    "    \"city\",\n",
    "    \"region\",\n",
    "    \"center_city_ua\",\n",
    "    \"center_city_en\",\n",
    "    \"isw_report_date\",\n",
    "    \"isw_date_tomorrow_datetime\",\n",
    "    \"isw_text_main\",\n",
    "    \"isw_keywords\",\n",
    "    \"isw_data_lemmatized\"\n",
    "    ]\n",
    "    df_isw[\"report_date\"] = df_isw[\"date\"].apply(lambda x: datetime.combine(x, datetime.min.time()))\n",
    "    df_isw[\"date_tomorrow_datetime\"] = df_isw[\"report_date\"].apply(lambda x: (x + timedelta(days=1)))\n",
    "\n",
    "    df_isw_short = df_isw[[\"report_date\", \"date_tomorrow_datetime\", \"keywords\", \"text_main\", \"data_lemmatized\",]]\n",
    "    weather_region_df = pd.merge(left=df_weather,right=df_region, left_on=\"city\", right_on=\"center_city_ua\")\n",
    "    df_isw_short = df_isw_short.copy().add_prefix('isw_')\n",
    "    df = weather_region_df.merge(df_isw_short,\n",
    "                            how = \"left\",\n",
    "                            left_on = \"day_datetime\",\n",
    "                            right_on = \"isw_date_tomorrow_datetime\")\n",
    "    df['day_datetime'] = pd.to_datetime(df['day_datetime'])\n",
    "    df_v2 = df.drop(fields_to_exlude, axis=1)\n",
    "    short_df_region = df_region[[\"region_alt\", \"region_id\"]]\n",
    "    df_v2 = df_v2.merge(short_df_region,\n",
    "                            how = \"left\",\n",
    "                            left_on = \"region_alt\",\n",
    "                            right_on = \"region_alt\")\n",
    "    df_v2[\"hour_conditions\"] = df_v2[\"hour_conditions\"].apply(lambda x: x.split(\",\")[0])\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    df_v2[\"hour_conditions_id\"] = label_encoder.fit_transform(df_v2[\"hour_conditions\"])\n",
    "    tmp_fields_to_exlude = [\n",
    "    \"city_latitude\",\n",
    "    \"city_longitude\",\n",
    "    \"city_address\",\n",
    "    \"city_timezone\",\n",
    "    \"city_tzoffset\",\n",
    "    \"day_feelslike\",\n",
    "    \"day_feelslikemax\",\n",
    "    \"day_feelslikemin\",\n",
    "    \"day_sunrise\",\n",
    "    \"day_sunset\",\n",
    "    \"day_precipprob\",\n",
    "    \"day_snow\",\n",
    "    \"day_snowdepth\",\n",
    "    \"day_windgust\",\n",
    "    \"day_windspeed\",\n",
    "    \"day_winddir\",\n",
    "    \"day_pressure\",\n",
    "    \"day_cloudcover\",\n",
    "    \"day_visibility\",\n",
    "    \"day_severerisk\",\n",
    "    \"day_sunriseEpoch\",\n",
    "    \"day_sunsetEpoch\",\n",
    "    \"day_conditions\",\n",
    "    \"day_description\",\n",
    "    \"day_icon\",\n",
    "    \"day_source\",\n",
    "    \"day_preciptype\",\n",
    "    \"day_stations\",\n",
    "    \"hour_feelslike\",\n",
    "    \"hour_preciptype\",\n",
    "    \"hour_conditions\",\n",
    "    \"region_alt\",\n",
    "    \"hour_solarenergy\",\n",
    "    \"hour_icon\",\n",
    "    \"hour_source\",\n",
    "    \"hour_stations\"\n",
    "    ]\n",
    "\n",
    "    df_work_v3 = df_v2.drop(tmp_fields_to_exlude, axis=1).fillna(method=\"ffill\")\n",
    "    print(df_work_v3[\"day_solarradiation\"])\n",
    "    df_work_v3[\"region_id_x\"] = df_work_v3[\"region_id_x\"].apply(lambda x: int(x))\n",
    "    df_work_v3[\"region_id_y\"] = df_work_v3[\"region_id_y\"].apply(lambda x: int(x))\n",
    "\n",
    "\n",
    "\n",
    "    df_work_v3_csr = scipy.sparse.csr_matrix(df_work_v3.values) # ERROR HERE\n",
    "\n",
    "    tfidf = pickle.load(open(\"model\"+ sep+ \"tfidf_transformer_v1.pkl\", \"rb\"))\n",
    "    cv = pickle.load(open(\"model\"+ sep+ \"count_vectorizer_v1.pkl\", \"rb\"))\n",
    "    word_count_vector = cv.transform(df['isw_data_lemmatized'].values.astype('U'))\n",
    "    tfidf_vector = tfidf.transform(word_count_vector)\n",
    "    df_all_features = scipy.sparse.hstack((df_work_v3_csr, tfidf_vector), format='csr')\n",
    "    return df_all_features\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\n",
    "def get_text_df(chosen_date):\n",
    "    html_file_name = save_page(\"https://understandingwar.org/backgrounder/russian-offensive-campaign-assessment-\"+str(chosen_date.strftime(\"%B\"))+\"-\"+str(chosen_date.day)+\"-2023\", str(chosen_date.date()))\n",
    "\n",
    "    divided_text_csv = text_processing(html_file_name)\n",
    "    remove(html_file_name)\n",
    "    divided_text = pd.read_csv(divided_text_csv)\n",
    "    remove(divided_text_csv)\n",
    "    divided_text['main_html_v2'] = divided_text['text_main'].apply(lambda x: remove_names_and_date(x))\n",
    "\n",
    "    pattern = \"\\[(\\d+)\\]\"\n",
    "    divided_text['main_html_v3'] = divided_text['main_html_v2'].apply(lambda x: re.sub(pattern, \"\", x))\n",
    "    divided_text['main_html_v4'] = divided_text['main_html_v3'].apply(lambda x: BeautifulSoup(x, features=\"html.parser\").text)\n",
    "    divided_text['main_html_v5'] = divided_text['main_html_v4'].apply(lambda x: re.sub(r'http(\\S+.*\\s)', \"\", x))\n",
    "    divided_text['main_html_v6'] = divided_text['main_html_v5'].apply(lambda x: re.sub(r'(©2022|©2023|2022|2023)', \"\", x))\n",
    "    divided_text['main_html_v7'] = divided_text['main_html_v6'].apply(lambda x: re.sub(r'\\n.{5,15}\\d:\\d.{0,9}\\n', \"\", x))\n",
    "    divided_text['main_html_v8'] = divided_text['main_html_v7'].apply(lambda x: re.sub('Appendix A – Satellite Imagery(.|\\n)+\\.', \"\", x)).apply(lambda x: re.sub('Click here to expand the map below.', \"\", x))\n",
    "    divided_text=divided_text.drop(['Unnamed: 0','main_html_v2','main_html_v3','main_html_v4','main_html_v5','main_html_v6','main_html_v7'],axis=1)\n",
    "    nltk.download()\n",
    "    words = [w for w in nltk.corpus.state_union.words() if w.isalpha()]\n",
    "\n",
    "    divided_text['main_html'] = divided_text['main_html_v8'].apply(lambda x: x.lower())\n",
    "    divided_text['main_html1'] = divided_text['main_html'].apply(lambda x: remove_special_characters(x))\n",
    "    divided_text['main_html2'] = divided_text['main_html1'].apply(lambda x: remove_any_punct(x))\n",
    "    divided_text['main_html3'] = divided_text['main_html2'].apply(lambda x: remove_one_letter_word(x))\n",
    "    nltk.download('wordnet')\n",
    "    divided_text['main_html4']=divided_text['main_html3'].apply(lambda x: convert_numbers(x)).apply(lambda x: remove_stopwords(x))\n",
    "    divided_text['data_stemmed'] = divided_text['main_html4'].apply(lambda x: stemming(x))\n",
    "    divided_text['data_lemmatized'] = divided_text['main_html4'].apply(lambda x: lemmatizing(x))\n",
    "    docs = divided_text['data_lemmatized'].tolist()\n",
    "    cv = CountVectorizer(min_df=0.98, max_df=2) #поміняв місцями мін та макс\n",
    "    word_count_vector = cv.fit_transform(docs)\n",
    "    with open(\"./model/count_vectorizer_v1.pkl\", 'wb') as handle:\n",
    "        pickle.dump(cv, handle)\n",
    "    tfidf_transformer = TfidfTransformer(smooth_idf=True,use_idf=True,)\n",
    "    tfidf_transformer.fit(word_count_vector)\n",
    "    with open(\"model/tfidf_transformer_v1.pkl\", 'wb') as handle:\n",
    "        pickle.dump(tfidf_transformer, handle)\n",
    "    df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names_out(), columns=[\"idf_weights\"])\n",
    "    df_idf.sort_values(by=['idf_weights'])\n",
    "    tf_idf_vector = tfidf_transformer.transform(word_count_vector)\n",
    "    tfidf = pickle.load(open(\"model/tfidf_transformer_v1.pkl\", \"rb\"))\n",
    "    cv = pickle.load(open(\"model/count_vectorizer_v1.pkl\", \"rb\"))\n",
    "    feature_names = cv.get_feature_names_out()\n",
    "    divided_text['keywords'] = divided_text['data_lemmatized'].apply(lambda x: conver_doc_to_vector(x, cv, tfidf))\n",
    "    return divided_text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong format! Try again\n",
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Богдан\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     165.6\n",
      "1     165.6\n",
      "2     165.6\n",
      "3     165.6\n",
      "4     165.6\n",
      "5     165.6\n",
      "6     165.6\n",
      "7     165.6\n",
      "8     165.6\n",
      "9     165.6\n",
      "10    165.6\n",
      "11    165.6\n",
      "12    165.6\n",
      "13     10.9\n",
      "14     10.9\n",
      "15     10.9\n",
      "16     10.9\n",
      "17     10.9\n",
      "18     10.9\n",
      "19     10.9\n",
      "20     10.9\n",
      "21     10.9\n",
      "22     10.9\n",
      "23     10.9\n",
      "Name: day_solarradiation, dtype: float64\n",
      "  (0, 0)\t15.6\n",
      "  (0, 1)\t6.0\n",
      "  (0, 2)\t11.1\n",
      "  (0, 3)\t1.7\n",
      "  (0, 4)\t53.7\n",
      "  (0, 5)\t0.3\n",
      "  (0, 6)\t4.17\n",
      "  (0, 7)\t165.6\n",
      "  (0, 8)\t14.2\n",
      "  (0, 9)\t7.0\n",
      "  (0, 10)\t0.69\n",
      "  (0, 11)\t14.0\n",
      "  (0, 12)\t48.82\n",
      "  (0, 13)\t3.4\n",
      "  (0, 18)\t35.6\n",
      "  (0, 19)\t14.8\n",
      "  (0, 20)\t72.0\n",
      "  (0, 21)\t1016.0\n",
      "  (0, 22)\t24.1\n",
      "  (0, 23)\t99.9\n",
      "  (0, 24)\t565.0\n",
      "  (0, 25)\t6.0\n",
      "  (0, 26)\t10.0\n",
      "  (0, 27)\t16.0\n",
      "  (0, 28)\t16.0\n",
      "  :\t:\n",
      "  (23, 1)\t9.3\n",
      "  (23, 2)\t10.3\n",
      "  (23, 3)\t7.2\n",
      "  (23, 4)\t81.5\n",
      "  (23, 5)\t12.0\n",
      "  (23, 6)\t87.5\n",
      "  (23, 7)\t10.9\n",
      "  (23, 8)\t0.9\n",
      "  (23, 10)\t0.72\n",
      "  (23, 11)\t10.2\n",
      "  (23, 12)\t81.64\n",
      "  (23, 13)\t7.2\n",
      "  (23, 14)\t0.6\n",
      "  (23, 15)\t95.2\n",
      "  (23, 18)\t25.2\n",
      "  (23, 19)\t10.4\n",
      "  (23, 20)\t73.1\n",
      "  (23, 21)\t1005.0\n",
      "  (23, 22)\t13.6\n",
      "  (23, 23)\t100.0\n",
      "  (23, 24)\t19.0\n",
      "  (23, 26)\t10.0\n",
      "  (23, 27)\t16.0\n",
      "  (23, 28)\t16.0\n",
      "  (23, 29)\t1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#if __name__ == \"__main__\":\n",
    "label_encoder = pickle.load(open(\"model\"+sep+\"weather_conditions_label_encoder.pkl\", \"rb\"))\n",
    "df_regions = pd.read_csv(\"data\"+sep+\"weather_alarms_regions\"+sep+\"regions.csv\", sep=\",\")\n",
    "\n",
    "chosen_date = get_date_from_user()\n",
    "text_df = get_text_df(chosen_date - timedelta(days=1))\n",
    "text_df[\"date\"] = chosen_date.date() - timedelta(days=1)\n",
    "region_df = get_region_from_user()\n",
    "weather_forecast_df = weather.vectorize(weather.get_weather_forecast(chosen_date.isoformat(), region_df.at[\"center_city_en\"]+\",UA\"))\n",
    "weather_forecast_df[\"day_datetime\"] = pd.to_datetime(weather_forecast_df[\"day_datetime\"])\n",
    "weather_forecast_df[\"city\"] = weather_forecast_df[\"city_resolvedAddress\"].apply(lambda x: x.split(\",\")[0])\n",
    "weather_forecast_df[\"city\"] = weather_forecast_df[\"city\"].replace(\"Хмельницька область\", \"Хмельницький\")\n",
    "input_df = merge_weather_isw_region(weather_forecast_df, text_df, pd.DataFrame(region_df).transpose())\n",
    "print(input_df)\n",
    "\n",
    "file_path = \"model\" + sep + \"8_logistic_regression_v3.pkl\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "scipy.sparse._csr.csr_matrix"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(input_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.56000000e+01, 6.00000000e+00, 1.11000000e+01, ...,\n        9.37799979e-03, 3.75119992e-02, 1.40669997e-02],\n       [1.56000000e+01, 6.00000000e+00, 1.11000000e+01, ...,\n        9.37799979e-03, 3.75119992e-02, 1.40669997e-02],\n       [1.56000000e+01, 6.00000000e+00, 1.11000000e+01, ...,\n        9.37799979e-03, 3.75119992e-02, 1.40669997e-02],\n       ...,\n       [1.13000000e+01, 9.30000000e+00, 1.03000000e+01, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [1.13000000e+01, 9.30000000e+00, 1.03000000e+01, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [1.13000000e+01, 9.30000000e+00, 1.03000000e+01, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df.toarray()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf=pickle.load(open(f\"{MODEL_FOLDER}/8_logistic_regression_v3.pkl\", \"rb\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "schedule = clf.predict(input_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "print(schedule)\n",
    "print(\"1\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
